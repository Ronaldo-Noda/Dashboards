Data Science Academy

Capitulo 11 

Estatística Fundamental para Data Science

A Estatística lida com coleta, análise, interpretação, apresentação e organização de dados.
Extrair informações permitindo a tomada de decisão e elaboração de previsões em situações de incerteza.

Pode ser dividida em duas áreas principais: Estatística Descritiva e a Estatística inferencial.

- Estatística Descritiva: organização, resumo e apresentação dos dados de maneira eficiente, utilizando gráficos, tabelas e medidas numéricas como média, mediana, moda, variância e desvio padrão.

- Estatística Inferencial: utiliza técnicas e métodos para fazer generalizações e previsões a partir de dados amostrais. Envolve o uso de testes, hipóteses, intervalos de confiança e análise de regressão.

Normalmente um Analista de Dados trabalha com Estatística Descritiva no seu dia a dia, enquanto um Cientista de Dados trabalha com Estatística Descritiva e Estatística Inferencial no seu dia a dia.

Data Science é um campo que utiliza técnicas e métodos estatísticos, programação de computadores, conhecimento da área de negócio específico para extrair informações e conhecimentos a partir de dados brutos.

- Tabelas  Gráficos: Tabelas de frequencia, histogramas, gráfico de barras, graficos de setores, graficos de dispersão e gráficos de linhas são algumas ferramentas utilizadas para presentar os dados de forma visual e fácil de interpretar
- Medidas de tendência central: Média, mediana e moda são usadas para descrever o "centro" dos dados, fornecendo uma idéia geral do valor central em torno do qual os dados estão distribuidos.
- Medidas de dispersão: Virância, desvio padrão e amplitusw são usados para quantificar a dispersão ou variabilidade dos dados, fornecendo informações sobre a consistência e a diferenciça entre os valores observados.
- Medidas de posição: Quartis, percentis, e outros valores que indicam a posição relativa de um valor especifico dentro do conjunto de dados.

O Power BI é uma ótima ferramenta para Estatística Descritiva, permitindo calcular estatísticas de forma rápida e simples, além da criação de graficos e tabelas.

Estatóstica Inferencial é um conjunto de tecnicas e métodos usados para fazer generalização e previsões sobre uma pontuação com base em informações obtidas de uma amostra dessa população. A Estatística Inferencial permite estimar caracteristicas da população, testar hipóteses e fazer previsões. Algumas das principais técnicas de Estatísticas Inferencial incluem:

- Estimação: Estimação pontual e intervalar são usadas para estimar parâmetros populacionais, como média ou a proposrção, com base em dados amostrais e um grau de incerteza associado.
Testes de hipóteses: São usados para testar afirmações ou suposições, como camparar médias entre dois grupos ou verificar se uma proporção significamente diferente de um valor esperado.
- Análise de regressão: A análise a regressão é usada para modelar a relação entre uma variável dependente e uma ou mais variáveis independentes, permitindo prever valores futuros ou identificar variáveis que impactam o resultado de interesse.
- Análise de variância (ANOVA): A ANOVA é uma técnica usada para comparar as médias de dois ou mais grupos, verificando se há diferença significativas entre eles.
- Modelos probabilísticos e análise de séries temporais: São usados para analisar e modelar eventos aleatórios e a evolução de variáveis ao longo do tempo.

O Power BI NÃO é uma ferramenta para Estatística Inferencial. Ou seja, o Power BI é uma ferramenta de Business Intelligence(onde usamos Estatística Descritiva) não de Data Science (onde  usamos  Estatística  Inferencial).  Para  Estatística  Inferencial  devemos  usar  ferramentas adequadas como pacotes das Linguagem Python ou R, IBM SPSS, SAS entre outras.

- Análise preditiva: Usando técnicas de aprendizado de uma máquina e análise estatística para prever eventos futuros, como demanda do cliente, falhas de equipamentos ou resultados eleitorais.
- Análise de sentimentos: Analisando o conteúdo de redes sociais, avaliações e comentários dos clientes para entender o sentimentos do público em relação a produtos, serviços ou eventos.
- Detecção de fraudes: Identificando atividades suspeitas e padrões de comportamento anormal em transações financeiras, comunicações ou registros de acesso.
- Análise de risco: Avaliando riscos e incertezas em setores como finanças, seguros e saúde, usando dados históricos e em tempo real para modelar e prever possiveis resultados.
- Recomendação personalizada: Desenvolvendo sistemas de recomendação que fornecem conteúdo, produtos ou serviços personalizados com base no comportamento passado e nas preferências dos usuários.
- Otimização da cadeia de suprimentos: Analisando dados dew inventário, logística e vendas para melhorar a eficiência, reduzir custos e prever necessidades futuras.

População, também conhecida como população-alvo ou universo, é o conjunto completo de elementos ou unidades de interesse em um estudo ou pesquisa. A população pode incluir pessoas, animais, objetos ou eventos e pode ser finita ou infinita, dependendo do contexto. Ao analisar a população, estamos interessados em suas características e propriedades, como média, proporção ou correlação.

Amostra é um subconjunto da população que é selecionado para representa-la em um estudo ou pesquisa. Em muitos casos é impraticável ou impossível coletar dados de toda a população devido a restrições de tempo, custo ou outros recursos, Por isso, os pesquisadores selecionam uma amostra que seja representativa da população para realizar a análise.
A  amostra  deve  ser  selecionada  usando  métodos  adequados  de  amostragem,  como amostragem  aleatória  simples,  estratificada  ou  por  conglomerados,  para  garantir  que  as características da população sejam adequadamente refletidas e que as inferências feitas a partir da amostra sejam válidas.

Amostragem Probabilística:
Nas técnicas de amostragem probabilística, cada elemento da população tem uma chance conhecida e não nula de ser selecionado para a amostra. Essas técnicas geralmente resultam em amostras mais representativas e permitem o cálculo de medidas de incerteza, como margem de erro e intervalos de confiança. As principais técnicas de amostragem probabilística incluem:
- Amostragem aleatória simples: Cada elemento da população tem igual probabilidade de ser selecionado. É como um sorteio onde todos os elementos têm a mesma chance de serem escolhidos.
- Amostragem sistemática: Os elementos da população são selecionados em intervalos fixos, a partir de um ponto de partida aleatório. Por exemplo, a cada 10 elementos, um é escolhido.
- Amostragem  estratificada:  A  população  é  dividida  em  subgrupos  homogêneos, chamados  estratos,  e  uma  amostra  aleatória  é  selecionada  de  cada  estrato.  Isso garante que todos os segmentos da população sejam adequadamente representados na amostra.
- Amostragem por conglomerados: A população é dividida em grupos heterogêneos, chamados conglomerados. Alguns conglomerados são selecionados aleatoriamente e todos  os  elementos  desses  conglomerados  são  incluídos  na  amostra.  Os conglomerados  podem  ser  selecionados  com  base  em  critérios  geográficos, demográficos ou outros

Amostragem Não Probabilística:
Nas técnicas de amostragem não probabilística, a seleção dos elementos da população não é baseada na probabilidade. Essas técnicas são mais fáceis e rápidas de serem executadas, mas podem resultar em amostras menos representativas e não permitem o cálculo de medidas de incerteza. As principais técnicas de amostragem não probabilística incluem:
- Amostragem por conveniência: A seleção dos elementos é baseada na facilidade de acesso e na disponibilidade. Essa técnica pode ser enviesada, já que nem todos os elementos têm a mesma chance de serem selecionados.
- Amostragem por julgamento: O pesquisador seleciona os elementos da amostra com base em seu conhecimento e critério. Embora possa ser útil em casos específicos, essa técnica é suscetível a vieses e erros de julgamento.
- Amostragem  por  quotas:  Semelhante  à  amostragem  estratificada,  a  população  é dividida em subgrupos. No entanto, os elementos são selecionados de forma não aleatória,  com  base  em  características  específicas,  até  que  uma  quota  pré-determinada seja atingida.
A amostragem é uma parte fundamentaldo trabalho em projetos de Ciência de Dados.

Parâmetro
Um parâmetro é uma medida numérica que descreve uma característica específica de uma população. Ele é um valor fixo e desconhecido, já que geralmente não é possível analisar todos os elementos da população. Os parâmetros são frequentemente representados por letras gregas, como μ (média populacional) e σ (desvio padrão populacional). Os parâmetros fornecem informações valiosas sobre a população e são o objetivo final de muitas análises estatísticas.

Estatística
Uma estatística é uma medida numérica calculada a partir de uma amostraselecionada da  população.  As  estatísticas  são  usadas  para  estimar  parâmetros  populacionais  e  são representadas por letras latinas, como x̄(média amostral) e s (desvio padrão amostral). Uma estatística é uma variável aleatória, já que seu valor varia de uma amostra para outra, e é possível calcular intervalos de confiança e margens de erro em torno dela.

Em resumo, um parâmetro é uma medida que descreve uma característica da população,enquanto  uma  estatística  é  uma  medida  calculada  a  partir  de  uma  amostra  para  estimar  o parâmetro correspondente. A diferença entre os dois reside no fato de que os parâmetros são valores fixos e desconhecidos relacionados à população, enquanto as estatísticas são valores variáveis e conhecidos obtidos a partir de amostras.

Dados primários são informações coletadas diretamente pelo pesquisador ou sua equipe para responder a uma pergunta específica de pesquisa ou atender a um objetivo específico. Esses dados são coletados pela primeira vez e são originais, ou seja, não foram utilizados em pesquisas anteriores. Os dados primários são geralmente coletados por meio de métodos como entrevistas, questionários, observações, experimentos ou outros meios diretos de coleta de informações.
Vantagens dos Dados Primários: Confiabilidade, qualidade, controle das informações, acertabilidade nos resultados, dados atualizados.
Desvantagens: Alto custo, demanda maior tempo, equipe grande.

Dados secundários são informações já coletadas e disponíveis, que foram obtidas ou geradas  em  pesquisas  ou  projetos  anteriores,  ou  que  são  coletadas  regularmente  por organizações ou agências. Esses dados não são coletados especificamente para a pergunta de pesquisa  em  questão,  mas  podem  ser  aplicados  ou  reutilizados  para responder  a  novas perguntas.  Os  dados  secundários  podem  incluir  relatórios  de  pesquisa,  estudos  acadêmicos, registros administrativos, dados de censo, informações financeiras e estatísticas governamentais, entre outros.As vantagens dos dados secundários incluem economia de tempo e recursos, já que as informações já estão disponíveis, e a possibilidade de acessar grandes conjuntos de dados que seriam difíceis ou impossíveis de coletar diretamente. No entanto, os dados secundários podem não ser totalmente relevantes ou atualizados para a pesquisa em questão e podem ter limitações em termos de qualidade e confiabilidade.
Vantagens dos Dados Secundários: Baixo custo, rapidez, existência de fontes, diversidade de informações.
Desvantagens: Falta de controle, dados inadequados, diversidade na classificação dos dados, dados desatualizados, fontes não confiaveis, dificuldade de reproduzir um estudo obtendo os mesmo resultados.


Observações, também conhecidas como casos ou registros, são as unidades individuais de informação em um conjunto de dados. Cada observação representa uma instância única de um objeto, pessoa, evento ou situação que foi medido ou registrado. Em um conjunto de dados, as observações são geralmente organizadas em linhas. Por  exemplo,  se  você  estiver  conduzindo  uma  pesquisa  sobre  a  altura  e  o  peso  de estudantes em uma escola, cada estudante seria uma observação individual no conjunto de dados.
Variáveis, por outro lado, são as características ou atributos medidos ou registrados para cada  observação.  As  variáveis  descrevem  as  propriedades,  qualidades  ou  quantidades  que variam entre as observações. Em um conjunto de dados, as variáveis são geralmente organizadas em colunas.Continuando com o exemplo anterior, a altura e o peso seriam variáveis no conjunto de dados dos estudantes. Cada estudante (observação) teria um valor específico para a altura e outro para o peso (variáveis).

		variáveis ---> idade	sexo 	peso	altura

observações:
estudante 1			14	 M	60	1,67
estudante 2
estudante 3
estudante 4

Os dados podem conter variáveis:
Qualitativas nominais e ordinais
Quantitativas discretas e contínuas

Qualitativas: utilizam termos descritivos para descrever algo de interesse. Ex: cor dos olhos, estado civil, religião, gênero, grau de escolaridade, classe social, tipo sanguíneo, cor da pele, etc...
Quantitativas: são representadas  por  valores  numéricos  que  podem  ser  contados  ou medidos. Ex: número de crianças em uma sala de aula, peso do corpo humano, idade, número de filhos, etc...

•Qualitativas nominais(não há uma ordem natural), como, por exemplo, o sexo de uma pessoa. 
•Qualitativas  ordinais(possuem uma ordem natural), como, por exemplo, o índice de aprovação de um político: péssimo, ruim, regular, bom ou ótimo
•Quantitativas  discretas(os possíveis valores são contáveis), como o número de alunos em uma sala ou o número de acessos por dia ao portal da Data Science Academy.
•Quantitativas  contínuas(podem  ser  observados  quaisquer  valores  dentro  de  um intervalo), como a altura ou peso de uma pessoa

Um dado classificado como "idade” pode ser quantitativo.Ex:. 11, 15, 18, 25, 42 anos.
Entretanto, se esse dado for informado por ”faixa etária” ele é qualitativo (ordinal).Ex: 0 –5 anos, 6 –12 anos, 13 –18 anos, 19 –28 anos

Média
A média é a soma de todos os valores de um conjunto de dados dividida pelo número total de valores. É uma das medidas de tendência central mais comuns e frequentemente usada para representar o valor "típico" de um conjunto de dados. A média pode ser afetada por valores extremos (outliers) e pode não ser a melhor representação do centro dos dados em tais casos.
Mediana
A mediana é o valor que separa um conjunto de dados ordenado em duas metades iguais. Se o número total de valores no conjunto de dados é ímpar, a mediana é o valor do meio. Se o número total de valores é par, a mediana é a média dos dois valores centrais. A mediana é menos sensível a valores extremos e pode ser uma medida mais representativa do centro dos dados quando a distribuição é assimétrica ou contém outliers(valores extremos).
Moda
A  moda  é  o  valor  que  ocorre  com  maior  frequência  em  um  conjunto  de  dados.  Um conjunto  de  dados  pode  ter  nenhuma  moda,  uma  moda  (unimodal) ou  várias  modas (multimodal). A moda pode ser usada para dados numéricos ou categóricos e é uma medida útil da tendência central, especialmente quando a média e a mediana não são aplicáveis ou não fornecem uma representação adequada do centro dos dados.

As medidas de dispersão são estatísticas que quantificam a dispersão, a variabilidade ou a dispersão dos valores em um conjunto de dados. Elas ajudam a entender o quão dispersos estão os valores em torno da medida central (como a média). As duas medidas de dispersão mais comuns são a variância e o desvio padrão.

Variância
A variância é uma medida que indica o quanto os valores em um conjunto de dados variam  em  torno  da  média.  Uma  variância  maior  indica  uma  maior  dispersão  dos  valores, enquanto uma variância menor sugere que os valores estão mais próximos da média. A variância é calculada como a média dos quadrados das diferenças entre cada valor e a média do conjunto de dados.
Desvio padrão
O desvio padrão é a raiz quadrada da variância e também mede a dispersão dos valores em um conjunto de dados. Ele é expresso na mesma unidade de medida dos valores originais, o que o torna mais fácil de interpretar em comparação com a variância. Um desvio padrão maior indica maior variabilidade dos valores, enquanto um desvio padrão menor sugere que os valores estão mais próximos da média.

A variância  e  o  desvio  padrão  são  medidas  de  dispersão  que  ajudam  a  quantificar  a dispersão ou a variabilidade dos valores em um conjunto de dados em torno da média. Essas medidas são úteis para entender a consistência dos dados e para comparar a dispersão entre diferentes conjuntos de dados ou variáveis.

O coeficiente de variação (CV), também conhecido como coeficiente de variação relativa, é uma medida estatística que expressa a relação entre o desvio padrão e a média de um conjunto de dados.
O coeficiente de variação é frequentemente expresso como um percentual.
O coeficiente de variação é especialmente útil quando se deseja comparar a dispersão de dois ou mais conjuntos de dados que possuem diferentes escalas ou unidades de medida. 
Fórmula para calcular o coeficiente de variação (CV):
CV = (Desvio Padrão / Média) × 100
Um CV menor indica que os dados são menos dispersos em relação à média, enquanto um CV maior indica que os dados são mais dispersos.

As medidas de posição relativa são estatísticas que descrevem a posição de um valor específico  em  relação  a  outros  valores  em  um  conjunto  de  dados.  Essas  medidas  fornecem informações sobre a posição de um valor dentro da distribuição dos dados e ajudam a entender o contexto dos dados. As medidas de posição relativa mais comuns são os percentis, quartis e z-score(Escore z).
Percentis
Os percentis são medidas que dividem um conjunto de dados ordenado em 100 partes iguais. O percentil de um valor específico indica a porcentagem de valores no conjunto de dados que são menores ou iguais a esse valor. Por exemplo, um valor no percentil 25 (P25) indica que 25% dos valores no conjunto de dados são menores ou iguais a esse valor. Os percentis são úteis para comparar a posição relativa de um valor dentro de diferentes conjuntos de dados e para entender a dispersão dos dados.
Quartis
Os quartissão medidas semelhantes aos percentis, mas dividem um conjunto de dados ordenado em quatro partes iguais. Existem três quartis: o primeiro quartil (Q1), o segundo quartil (Q2) e o terceiro quartil (Q3). Q1 corresponde ao percentil 25 (P25), Q2 correspondeà mediana (percentil 50 -P50) e Q3 corresponde ao percentil 75 (P75). Os quartis ajudam a entender a dispersão dos dados e a identificar a presença de valores extremos ou outliers.
Z-score (ou escore z)
O z-score é uma medida que expressa a posição relativa de um valor em relação à média e  ao  desvio  padrão  de  um  conjunto  de  dados.  Ele  indica  quantos  desvios  padrão  um  valor específico está acima ou abaixo da média do conjunto de dados. Um z-score positivo indica que o valor está acima da média, enquanto um z-score negativo indica que o valor está abaixo da média. Os z-scores são úteis para comparar a posição relativa de valores em diferentes conjuntos de dados e para identificar valores extremos ou outliers.

Análise Descritiva
A análise descritiva envolve a descrição e resumo dos dados por meio de medidas de tendência  central  (média,  mediana,  moda),  medidas  de  dispersão  (variância,  desvio  padrão, coeficiente de variação), e medidas de posição relativa (percentis, quartis, z-scores). Essa análise fornece uma visão geral dos dados e ajuda a entender sua distribuição e características básicas.

Análise de Séries Temporais
A análise de séries temporais envolve a análise de dados coletados ao longo do tempo para identificar padrões, tendências e ciclos. Essa análise pode incluir a decomposição da série temporal em componentes sazonais e de tendência, a aplicação de modelos autorregressivos e de médias móveis (ARIMA) e a previsão de valores futuros.

Análise de Agrupamento (Clusterização)
A análise de agrupamento é um método de aprendizado não supervisionado que agrupa observações  com  base  em  suas  características  e  semelhanças.  Existem  vários  algoritmos  de agrupamento, como k-means, agrupamento hierárquico e DBSCAN.

Análise de Componentes Principais (PCA)
A PCA é uma técnica de redução de dimensionalidadeque transforma um conjunto de dados  com  muitas  variáveis  correlacionadas  em  um  conjunto  de  dados  com  variáveis  não correlacionadas chamadas de componentes principais.














































